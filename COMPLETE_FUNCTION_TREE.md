# ðŸŒ³ FAIR-Agent Complete Function Tree
## From Start to End - Complete Execution Flow

---

# ðŸ“‹ Table of Contents
1. [System Startup Flow](#1-system-startup-flow)
2. [Query Processing Flow](#2-query-processing-flow)
3. [Baseline Calculation Flow](#3-baseline-calculation-flow)
4. [Evaluation Flow](#4-evaluation-flow)
5. [Complete Function Reference](#5-complete-function-reference)

---

# 1. SYSTEM STARTUP FLOW

## 1.1 Django Web Server Startup

```
USER COMMAND: python manage.py runserver
â”‚
â”œâ”€â–º Django Framework Initialization
â”‚   â”‚
â”‚   â”œâ”€â–º webapp/settings.py loads
â”‚   â”‚   â””â”€â–º INSTALLED_APPS includes 'fair_agent_app'
â”‚   â”‚
â”‚   â””â”€â–º webapp/urls.py loads
â”‚       â””â”€â–º Includes fair_agent_app.urls
â”‚
â”œâ”€â–º webapp/fair_agent_app/apps.py
â”‚   â”‚
â”‚   â””â”€â–º FairAgentAppConfig.ready()
â”‚       â””â”€â–º Triggers app initialization
â”‚
â””â”€â–º webapp/fair_agent_app/services.py
    â”‚
    â””â”€â–º FairAgentService (Class-level initialization)
        â”‚
        â”œâ”€â–º _orchestrator = None (lazy initialization)
        â”œâ”€â–º _evaluators = None (lazy initialization)
        â””â”€â–º _baseline_scores_path = "results/baseline_scores.json"
```

**Purpose**: Start Django web server and prepare FAIR-Agent services

---

## 1.2 First Query - Lazy Initialization

```
FIRST USER QUERY arrives at http://localhost:8000/api/query/
â”‚
â””â”€â–º webapp/fair_agent_app/views.py
    â”‚
    â””â”€â–º query_api(request) [Line 394]
        â”‚
        â””â”€â–º FairAgentService.process_query(query, session_id, model_name)
            â”‚
            â”œâ”€â–º @classmethod _ensure_initialized(cls) [Line 35]
            â”‚   â”‚
            â”‚   â”œâ”€â–º Check if cls._orchestrator is None
            â”‚   â”‚   â””â”€â–º YES â†’ Initialize orchestrator
            â”‚   â”‚
            â”‚   â”œâ”€â–º src/agents/orchestrator.py
            â”‚   â”‚   â””â”€â–º Orchestrator.__init__(finance_config, medical_config)
            â”‚   â”‚       â”‚
            â”‚   â”‚       â”œâ”€â–º Initialize FinanceAgent [Line 54]
            â”‚   â”‚       â”‚   â”‚
            â”‚   â”‚       â”‚   â””â”€â–º src/agents/finance_agent.py
            â”‚   â”‚       â”‚       â””â”€â–º FinanceAgent.__init__(model_name)
            â”‚   â”‚       â”‚           â”‚
            â”‚   â”‚       â”‚           â”œâ”€â–º ModelRegistry.get_domain_recommended_model('finance')
            â”‚   â”‚       â”‚           â”‚   â””â”€â–º Returns: 'llama3.2:latest'
            â”‚   â”‚       â”‚           â”‚
            â”‚   â”‚       â”‚           â”œâ”€â–º Initialize Enhancement Systems:
            â”‚   â”‚       â”‚           â”‚   â”œâ”€â–º ResponseEnhancer() [safety/disclaimer_system.py]
            â”‚   â”‚       â”‚           â”‚   â”œâ”€â–º RAGSystem() [evidence/rag_system.py]
            â”‚   â”‚       â”‚           â”‚   â”œâ”€â–º ChainOfThoughtIntegrator() [reasoning/cot_system.py]
            â”‚   â”‚       â”‚           â”‚   â””â”€â–º InternetRAGSystem() [data_sources/internet_rag.py]
            â”‚   â”‚       â”‚           â”‚
            â”‚   â”‚       â”‚           â””â”€â–º OllamaClient() [utils/ollama_client.py]
            â”‚   â”‚       â”‚               â””â”€â–º Check Ollama availability (http://localhost:11435)
            â”‚   â”‚       â”‚
            â”‚   â”‚       â”œâ”€â–º Initialize MedicalAgent [Line 55]
            â”‚   â”‚       â”‚   â””â”€â–º Similar initialization as FinanceAgent
            â”‚   â”‚       â”‚
            â”‚   â”‚       â””â”€â–º Initialize RAGSystem for orchestrator [Line 56]
            â”‚   â”‚
            â”‚   â”œâ”€â–º Check if cls._evaluators is None
            â”‚   â”‚   â””â”€â–º YES â†’ Initialize evaluators [Line 50-75]
            â”‚   â”‚       â”‚
            â”‚   â”‚       â”œâ”€â–º FaithfulnessEvaluator()
            â”‚   â”‚       â”œâ”€â–º AdaptabilityEvaluator()
            â”‚   â”‚       â”œâ”€â–º CalibrationEvaluator()
            â”‚   â”‚       â”œâ”€â–º RobustnessEvaluator()
            â”‚   â”‚       â”œâ”€â–º SafetyEvaluator()
            â”‚   â”‚       â””â”€â–º InterpretabilityEvaluator()
            â”‚   â”‚
            â”‚   â””â”€â–º _check_baseline_scores() [Line 124]
            â”‚       â”‚
            â”‚       â”œâ”€â–º Check if results/baseline_scores.json exists
            â”‚       â”‚
            â”‚       â””â”€â–º NO â†’ Calculate baseline [Line 157-165]
            â”‚           â”‚
            â”‚           â””â”€â–º src/evaluation/baseline_evaluator.py
            â”‚               â””â”€â–º BaselineEvaluator.run_baseline_evaluation(num_queries_per_domain=3)
            â”‚                   â””â”€â–º Tests 9 queries (3 Ã— 3 domains)
            â”‚                       â””â”€â–º Saves results/baseline_scores.json
            â”‚
            â””â”€â–º System Ready! âœ…
```

**Purpose**: Initialize all FAIR-Agent components on first query (lazy loading)

---

# 2. QUERY PROCESSING FLOW

## 2.1 Complete User Query Flow

```
USER QUERY: "What is the best investment strategy for retirement?"
â”‚
â”œâ”€â–º webapp/fair_agent_app/views.py
â”‚   â”‚
â”‚   â””â”€â–º query_api(request) [Line 394]
â”‚       â”‚
â”‚       â”œâ”€â–º Extract: query, session_id, model_name from request.POST
â”‚       â”‚
â”‚       â””â”€â–º FairAgentService.process_query(query, session_id, model_name)
â”‚
â””â”€â–º webapp/fair_agent_app/services.py
    â”‚
    â””â”€â–º process_query(cls, query, session_id, model_name) [Line 195]
        â”‚
        â”œâ”€â–º STEP 1: Ensure Initialized
        â”‚   â””â”€â–º cls._ensure_initialized()
        â”‚
        â”œâ”€â–º STEP 2: Reinitialize if model changed
        â”‚   â””â”€â–º IF model_name != current_model:
        â”‚       â””â”€â–º cls._reinitialize_agents_with_model(model_name)
        â”‚
        â”œâ”€â–º STEP 3: Process Query
        â”‚   â”‚
        â”‚   â””â”€â–º cls._orchestrator.process_query(query)
        â”‚       â”‚
        â”‚       â””â”€â–º src/agents/orchestrator.py
        â”‚           â”‚
        â”‚           â””â”€â–º process_query(self, query) [Line 80]
        â”‚               â”‚
        â”‚               â”œâ”€â–º STEP 3.1: Classify Query Domain
        â”‚               â”‚   â”‚
        â”‚               â”‚   â””â”€â–º _classify_query_domain(query) [Line 124]
        â”‚               â”‚       â”‚
        â”‚               â”‚       â”œâ”€â–º Finance Keywords Check:
        â”‚               â”‚       â”‚   finance_keywords = ['investment', 'portfolio', 'stock', 
        â”‚               â”‚       â”‚                       'bond', 'retirement', 'savings', ...]
        â”‚               â”‚       â”‚   finance_score = count(keyword in query.lower())
        â”‚               â”‚       â”‚
        â”‚               â”‚       â”œâ”€â–º Medical Keywords Check:
        â”‚               â”‚       â”‚   medical_keywords = ['health', 'disease', 'symptom',
        â”‚               â”‚       â”‚                       'treatment', 'medication', ...]
        â”‚               â”‚       â”‚   medical_score = count(keyword in query.lower())
        â”‚               â”‚       â”‚
        â”‚               â”‚       â”œâ”€â–º Decision Logic:
        â”‚               â”‚       â”‚   IF finance_score >= 2 OR medical_score >= 2:
        â”‚               â”‚       â”‚       â†’ Strong domain match
        â”‚               â”‚       â”‚   ELIF finance_score >= 1 OR medical_score >= 1:
        â”‚               â”‚       â”‚       â†’ Weak domain match
        â”‚               â”‚       â”‚   ELSE:
        â”‚               â”‚       â”‚       â†’ Unknown/Cross-domain
        â”‚               â”‚       â”‚
        â”‚               â”‚       â””â”€â–º Returns: 'finance' (score: 2 - 'investment', 'retirement')
        â”‚               â”‚
        â”‚               â”œâ”€â–º STEP 3.2: Route to Finance Agent
        â”‚               â”‚   â”‚
        â”‚               â”‚   â””â”€â–º _handle_finance_query(query) [Line 154]
        â”‚               â”‚       â”‚
        â”‚               â”‚       â””â”€â–º src/agents/finance_agent.py
        â”‚               â”‚           â”‚
        â”‚               â”‚           â””â”€â–º FinanceAgent.query(question, context, return_confidence)
        â”‚               â”‚               â”‚
        â”‚               â”‚               â”œâ”€â–º STEP A: Retrieve Evidence (RAG)
        â”‚               â”‚               â”‚   â”‚
        â”‚               â”‚               â”‚   â””â”€â–º self.rag_system.retrieve_evidence(query, domain='finance', top_k=3)
        â”‚               â”‚               â”‚       â”‚
        â”‚               â”‚               â”‚       â””â”€â–º src/evidence/rag_system.py
        â”‚               â”‚               â”‚           â”‚
        â”‚               â”‚               â”‚           â””â”€â–º RAGSystem.retrieve_evidence() [Line 421]
        â”‚               â”‚               â”‚               â”‚
        â”‚               â”‚               â”‚               â”œâ”€â–º EvidenceDatabase.search_sources(query, top_k=3, semantic=True)
        â”‚               â”‚               â”‚               â”‚   â”‚
        â”‚               â”‚               â”‚               â”‚   â””â”€â–º _semantic_search(query_embedding, top_k) [Line 211]
        â”‚               â”‚               â”‚               â”‚       â”‚
        â”‚               â”‚               â”‚               â”‚       â”œâ”€â–º Get query embedding:
        â”‚               â”‚               â”‚               â”‚       â”‚   self.embedding_model.encode(query)
        â”‚               â”‚               â”‚               â”‚       â”‚   â†’ 768-dimensional vector
        â”‚               â”‚               â”‚               â”‚       â”‚
        â”‚               â”‚               â”‚               â”‚       â”œâ”€â–º Calculate cosine similarity:
        â”‚               â”‚               â”‚               â”‚       â”‚   similarity = (A Â· B) / (||A|| Ã— ||B||)
        â”‚               â”‚               â”‚               â”‚       â”‚   For each evidence source
        â”‚               â”‚               â”‚               â”‚       â”‚
        â”‚               â”‚               â”‚               â”‚       â”œâ”€â–º Dynamic threshold:
        â”‚               â”‚               â”‚               â”‚       â”‚   threshold = mean(top_5) - 0.5 Ã— std(top_5)
        â”‚               â”‚               â”‚               â”‚       â”‚
        â”‚               â”‚               â”‚               â”‚       â””â”€â–º Return top 3 sources with similarity â‰¥ threshold
        â”‚               â”‚               â”‚               â”‚
        â”‚               â”‚               â”‚               â””â”€â–º Returns: [EvidenceSource1, EvidenceSource2, EvidenceSource3]
        â”‚               â”‚               â”‚
        â”‚               â”‚               â”œâ”€â–º STEP B: Construct Evidence-Based Prompt
        â”‚               â”‚               â”‚   â”‚
        â”‚               â”‚               â”‚   â””â”€â–º _construct_prompt_with_evidence(question, evidence_sources) [Line 358]
        â”‚               â”‚               â”‚       â”‚
        â”‚               â”‚               â”‚       â”œâ”€â–º Format evidence:
        â”‚               â”‚               â”‚       â”‚   self.rag_system.format_evidence_for_prompt(evidence_sources)
        â”‚               â”‚               â”‚       â”‚   â†’ "[Source 1] Title: ...\nContent: ...\n"
        â”‚               â”‚               â”‚       â”‚
        â”‚               â”‚               â”‚       â””â”€â–º Build prompt:
        â”‚               â”‚               â”‚           """You are a financial expert. Use ONLY these sources:
        â”‚               â”‚               â”‚           {evidence_text}
        â”‚               â”‚               â”‚           
        â”‚               â”‚               â”‚           CRITICAL INSTRUCTIONS:
        â”‚               â”‚               â”‚           1. âœ… Base answer ONLY on evidence sources
        â”‚               â”‚               â”‚           2. âœ… Cite sources [Source X]
        â”‚               â”‚               â”‚           3. âœ… Use step-by-step reasoning
        â”‚               â”‚               â”‚           4. âœ… Express uncertainty where limited
        â”‚               â”‚               â”‚           
        â”‚               â”‚               â”‚           Question: {question}
        â”‚               â”‚               â”‚           """
        â”‚               â”‚               â”‚
        â”‚               â”‚               â”œâ”€â–º STEP C: Generate LLM Response (Ollama)
        â”‚               â”‚               â”‚   â”‚
        â”‚               â”‚               â”‚   â””â”€â–º self.ollama_client.generate() [Line 133]
        â”‚               â”‚               â”‚       â”‚
        â”‚               â”‚               â”‚       â””â”€â–º src/utils/ollama_client.py
        â”‚               â”‚               â”‚           â”‚
        â”‚               â”‚               â”‚           â””â”€â–º OllamaClient.generate(model, prompt, max_tokens=512, temp=0.7, top_p=0.9)
        â”‚               â”‚               â”‚               â”‚
        â”‚               â”‚               â”‚               â”œâ”€â–º POST http://localhost:11435/api/generate
        â”‚               â”‚               â”‚               â”‚   {
        â”‚               â”‚               â”‚               â”‚     "model": "llama3.2:latest",
        â”‚               â”‚               â”‚               â”‚     "prompt": "{evidence-based-prompt}",
        â”‚               â”‚               â”‚               â”‚     "temperature": 0.7,
        â”‚               â”‚               â”‚               â”‚     "top_p": 0.9,
        â”‚               â”‚               â”‚               â”‚     "max_tokens": 512
        â”‚               â”‚               â”‚               â”‚   }
        â”‚               â”‚               â”‚               â”‚
        â”‚               â”‚               â”‚               â”œâ”€â–º Temperature Sampling:
        â”‚               â”‚               â”‚               â”‚   probability(token) = softmax(logits / temperature)
        â”‚               â”‚               â”‚               â”‚   softmax(x_i) = exp(x_i) / Î£ exp(x_j)
        â”‚               â”‚               â”‚               â”‚
        â”‚               â”‚               â”‚               â”œâ”€â–º Nucleus Sampling (top_p=0.9):
        â”‚               â”‚               â”‚               â”‚   Select tokens until cumulative_prob â‰¥ 0.9
        â”‚               â”‚               â”‚               â”‚
        â”‚               â”‚               â”‚               â””â”€â–º Returns: base_answer (raw LLM text)
        â”‚               â”‚               â”‚
        â”‚               â”‚               â”œâ”€â–º STEP D: Enhancement Pipeline
        â”‚               â”‚               â”‚   â”‚
        â”‚               â”‚               â”‚   â””â”€â–º _enhance_with_systems(query, base_answer) [Line 219]
        â”‚               â”‚               â”‚       â”‚
        â”‚               â”‚               â”‚       â”œâ”€â–º Enhancement 1: Internet RAG
        â”‚               â”‚               â”‚       â”‚   â”‚
        â”‚               â”‚               â”‚       â”‚   â””â”€â–º self.internet_rag.enhance_finance_response(query, base_answer)
        â”‚               â”‚               â”‚       â”‚       â”‚
        â”‚               â”‚               â”‚       â”‚       â””â”€â–º src/data_sources/internet_rag.py
        â”‚               â”‚               â”‚       â”‚           â””â”€â–º InternetRAGSystem.enhance_finance_response()
        â”‚               â”‚               â”‚       â”‚               â”‚
        â”‚               â”‚               â”‚       â”‚               â”œâ”€â–º Tavily API Search (real-time)
        â”‚               â”‚               â”‚       â”‚               â”‚   â†’ Returns internet sources
        â”‚               â”‚               â”‚       â”‚               â”‚
        â”‚               â”‚               â”‚       â”‚               â””â”€â–º internet_boost = len(sources) Ã— 0.05 (max 0.15)
        â”‚               â”‚               â”‚       â”‚
        â”‚               â”‚               â”‚       â”œâ”€â–º Enhancement 2: Evidence Database
        â”‚               â”‚               â”‚       â”‚   â”‚
        â”‚               â”‚               â”‚       â”‚   â””â”€â–º self.rag_system.enhance_agent_response(response, query, 'finance')
        â”‚               â”‚               â”‚       â”‚       â”‚
        â”‚               â”‚               â”‚       â”‚       â””â”€â–º EvidenceIntegrator.enhance_response_with_evidence()
        â”‚               â”‚               â”‚       â”‚           â”‚
        â”‚               â”‚               â”‚       â”‚           â”œâ”€â–º Add citations: [Source X]
        â”‚               â”‚               â”‚       â”‚           â”œâ”€â–º evidence_coverage = cited_claims / total_claims
        â”‚               â”‚               â”‚       â”‚           â””â”€â–º citation_quality = avg(source.reliability_score)
        â”‚               â”‚               â”‚       â”‚
        â”‚               â”‚               â”‚       â””â”€â–º Enhancement 3: Safety System
        â”‚               â”‚               â”‚           â”‚
        â”‚               â”‚               â”‚           â””â”€â–º self.response_enhancer.enhance_response(response, query, 'finance')
        â”‚               â”‚               â”‚               â”‚
        â”‚               â”‚               â”‚               â””â”€â–º src/safety/disclaimer_system.py
        â”‚               â”‚               â”‚                   â”‚
        â”‚               â”‚               â”‚                   â””â”€â–º ResponseEnhancer.enhance_response()
        â”‚               â”‚               â”‚                       â”‚
        â”‚               â”‚               â”‚                       â”œâ”€â–º DisclaimerAnalyzer.analyze_response()
        â”‚               â”‚               â”‚                       â”‚   â”‚
        â”‚               â”‚               â”‚                       â”‚   â”œâ”€â–º risk_score = Î£(risk_keywords Ã— weight) / max
        â”‚               â”‚               â”‚                       â”‚   â””â”€â–º disclaimer_coverage = present / required
        â”‚               â”‚               â”‚                       â”‚
        â”‚               â”‚               â”‚                       â”œâ”€â–º Add financial disclaimer if missing
        â”‚               â”‚               â”‚                       â”‚   â†’ "âš ï¸ Important Financial Disclaimer..."
        â”‚               â”‚               â”‚                       â”‚
        â”‚               â”‚               â”‚                       â””â”€â–º safety_boost = 0.40
        â”‚               â”‚               â”‚
        â”‚               â”‚               â”œâ”€â–º STEP E: Add Structured Format
        â”‚               â”‚               â”‚   â”‚
        â”‚               â”‚               â”‚   â””â”€â–º _add_structured_format(response, evidence_sources) [Line 398]
        â”‚               â”‚               â”‚       â”‚
        â”‚               â”‚               â”‚       â”œâ”€â–º Check structure:
        â”‚               â”‚               â”‚       â”‚   has_steps = regex search for "Step X"
        â”‚               â”‚               â”‚       â”‚   has_citations = regex search for "[Source X]"
        â”‚               â”‚               â”‚       â”‚
        â”‚               â”‚               â”‚       â””â”€â–º Add if missing:
        â”‚               â”‚               â”‚           "## Financial Analysis
        â”‚               â”‚               â”‚           **Step 1:** {paragraph}
        â”‚               â”‚               â”‚           **Step 2:** {paragraph}
        â”‚               â”‚               â”‚           ## Evidence Sources Referenced
        â”‚               â”‚               â”‚           [Source 1] Title..."
        â”‚               â”‚               â”‚
        â”‚               â”‚               â”œâ”€â–º STEP F: Parse and Calculate Confidence
        â”‚               â”‚               â”‚   â”‚
        â”‚               â”‚               â”‚   â””â”€â–º _parse_finance_response(enhanced_answer, question, True, internet_count)
        â”‚               â”‚               â”‚       â”‚
        â”‚               â”‚               â”‚       â”œâ”€â–º Base Confidence Calculation:
        â”‚               â”‚               â”‚       â”‚   base_confidence = 0.30  # Conservative start
        â”‚               â”‚               â”‚       â”‚   + 0.10 if len(text) > 500
        â”‚               â”‚               â”‚       â”‚   + 0.05 if len(text) > 1000
        â”‚               â”‚               â”‚       â”‚   - 0.10 if len(text) < 200
        â”‚               â”‚               â”‚       â”‚   â†’ Capped: min(0.20, max(0.50, base))
        â”‚               â”‚               â”‚       â”‚
        â”‚               â”‚               â”‚       â”œâ”€â–º Enhancement Boosts:
        â”‚               â”‚               â”‚       â”‚   safety_boost = 0.40 (from ResponseEnhancer)
        â”‚               â”‚               â”‚       â”‚   evidence_boost = local + internet (max 0.35)
        â”‚               â”‚               â”‚       â”‚   reasoning_boost = interpretability_improvement
        â”‚               â”‚               â”‚       â”‚
        â”‚               â”‚               â”‚       â”œâ”€â–º Apply Chain-of-Thought Enhancement:
        â”‚               â”‚               â”‚       â”‚   â”‚
        â”‚               â”‚               â”‚       â”‚   â””â”€â–º ChainOfThoughtIntegrator.enhance_response_with_reasoning()
        â”‚               â”‚               â”‚       â”‚       â”‚
        â”‚               â”‚               â”‚       â”‚       â””â”€â–º src/reasoning/cot_system.py
        â”‚               â”‚               â”‚       â”‚           â”‚
        â”‚               â”‚               â”‚       â”‚           â””â”€â–º Generate reasoning chain
        â”‚               â”‚               â”‚       â”‚               â”œâ”€â–º Identify reasoning steps
        â”‚               â”‚               â”‚       â”‚               â”œâ”€â–º Add explicit logic flow
        â”‚               â”‚               â”‚       â”‚               â””â”€â–º reasoning_boost = improvement score
        â”‚               â”‚               â”‚       â”‚
        â”‚               â”‚               â”‚       â”œâ”€â–º Calibration (Evidence Quality Scaling):
        â”‚               â”‚               â”‚       â”‚   evidence_quality_factor = min(evidence_boost / 0.15, 1.0)
        â”‚               â”‚               â”‚       â”‚   scaled_safety = safety_boost Ã— (0.3 + 0.7 Ã— quality_factor)
        â”‚               â”‚               â”‚       â”‚   scaled_reasoning = reasoning_boost Ã— (0.4 + 0.6 Ã— quality_factor)
        â”‚               â”‚               â”‚       â”‚
        â”‚               â”‚               â”‚       â”œâ”€â–º Final Confidence:
        â”‚               â”‚               â”‚       â”‚   confidence = min(
        â”‚               â”‚               â”‚       â”‚       base + scaled_safety + evidence + scaled_reasoning,
        â”‚               â”‚               â”‚       â”‚       0.85  â† Calibration cap (never 100%)
        â”‚               â”‚               â”‚       â”‚   )
        â”‚               â”‚               â”‚       â”‚
        â”‚               â”‚               â”‚       â””â”€â–º Return: FinanceResponse
        â”‚               â”‚               â”‚           {
        â”‚               â”‚               â”‚             answer: str,
        â”‚               â”‚               â”‚             confidence_score: float,
        â”‚               â”‚               â”‚             reasoning_steps: List[str],
        â”‚               â”‚               â”‚             risk_assessment: str,
        â”‚               â”‚               â”‚             numerical_outputs: Dict[str, float],
        â”‚               â”‚               â”‚             safety_boost: 0.40,
        â”‚               â”‚               â”‚             evidence_boost: 0.25,
        â”‚               â”‚               â”‚             reasoning_boost: 0.12,
        â”‚               â”‚               â”‚             internet_boost: 0.10
        â”‚               â”‚               â”‚           }
        â”‚               â”‚               â”‚
        â”‚               â”‚               â””â”€â–º Returns: FinanceResponse to Orchestrator
        â”‚               â”‚
        â”‚               â””â”€â–º STEP 3.3: Construct Orchestrator Response
        â”‚                   â”‚
        â”‚                   â””â”€â–º OrchestratorResponse
        â”‚                       {
        â”‚                         primary_answer: finance_response.answer,
        â”‚                         confidence_score: finance_response.confidence_score,
        â”‚                         domain: 'finance',
        â”‚                         reasoning_steps: finance_response.reasoning_steps,
        â”‚                         risk_assessment: finance_response.risk_assessment,
        â”‚                         evidence_sources: [retrieved sources],
        â”‚                         enhancement_details: {boosts...}
        â”‚                       }
        â”‚
        â”œâ”€â–º STEP 4: Evaluate Response
        â”‚   â”‚
        â”‚   â””â”€â–º FairAgentService.evaluate_response(result, query, domain)
        â”‚       â”‚
        â”‚       â”œâ”€â–º Extract response details
        â”‚       â”‚
        â”‚       â”œâ”€â–º Faithfulness Evaluation:
        â”‚       â”‚   â””â”€â–º cls._evaluators['faithfulness'].evaluate_response(response, query)
        â”‚       â”‚       â†’ Score: 0.72
        â”‚       â”‚
        â”‚       â”œâ”€â–º Adaptability Evaluation:
        â”‚       â”‚   â””â”€â–º cls._evaluators['adaptability'].evaluate_adaptability(response, query, domain)
        â”‚       â”‚       â†’ Score: 0.68
        â”‚       â”‚
        â”‚       â”œâ”€â–º Safety Evaluation:
        â”‚       â”‚   â””â”€â–º cls._evaluators['safety'].evaluate_safety(response, query, domain)
        â”‚       â”‚       â†’ Score: 0.75
        â”‚       â”‚
        â”‚       â”œâ”€â–º Interpretability Evaluation:
        â”‚       â”‚   â””â”€â–º cls._evaluators['interpretability'].evaluate_interpretability(response, query, domain)
        â”‚       â”‚       â†’ Score: 0.66
        â”‚       â”‚
        â”‚       â””â”€â–º Return: evaluation_metrics
        â”‚           {
        â”‚             'faithfulness': 0.72,
        â”‚             'adaptability': 0.68,
        â”‚             'safety': 0.75,
        â”‚             'interpretability': 0.66,
        â”‚             'fair_score': 0.70
        â”‚           }
        â”‚
        â”œâ”€â–º STEP 5: Format Response
        â”‚   â”‚
        â”‚   â””â”€â–º Build JSON response with:
        â”‚       â”œâ”€â–º answer: primary_answer
        â”‚       â”œâ”€â–º confidence: confidence_score
        â”‚       â”œâ”€â–º domain: detected domain
        â”‚       â”œâ”€â–º metrics: evaluation scores
        â”‚       â”œâ”€â–º reasoning_steps: list
        â”‚       â”œâ”€â–º evidence_sources: list
        â”‚       â””â”€â–º enhancement_boosts: {safety, evidence, reasoning, internet}
        â”‚
        â””â”€â–º STEP 6: Return to User
            â”‚
            â””â”€â–º JSON Response sent to frontend
                {
                  "status": "success",
                  "answer": "For retirement investing, consider...",
                  "confidence": 0.78,
                  "domain": "finance",
                  "metrics": {...},
                  "reasoning_steps": [...],
                  "evidence_sources": [...],
                  "timestamp": "2025-11-17T..."
                }
```

**Purpose**: Complete end-to-end query processing with evidence retrieval, LLM generation, enhancements, and evaluation

---

# 3. BASELINE CALCULATION FLOW

## 3.1 Manual Baseline Calculation

```
USER COMMAND: python scripts/run_baseline_evaluation.py --queries-per-domain 5
â”‚
â””â”€â–º scripts/run_baseline_evaluation.py
    â”‚
    â””â”€â–º main() [Line 28]
        â”‚
        â”œâ”€â–º Parse arguments:
        â”‚   â”œâ”€â–º --queries-per-domain (default: 5)
        â”‚   â”œâ”€â–º --output-file (default: results/baseline_scores.json)
        â”‚   â””â”€â–º --verbose
        â”‚
        â””â”€â–º src/evaluation/baseline_evaluator.py
            â”‚
            â””â”€â–º BaselineEvaluator.__init__() [Line 46]
                â”‚
                â”œâ”€â–º Initialize vanilla LLM client:
                â”‚   self.vanilla_client = OllamaClient()
                â”‚   â†’ NO RAG, NO enhancements, pure vanilla
                â”‚
                â”œâ”€â–º Initialize evaluators (same as FAIR uses):
                â”‚   â”œâ”€â–º FaithfulnessEvaluator(use_embeddings=False)
                â”‚   â”œâ”€â–º AdaptabilityEvaluator()
                â”‚   â”œâ”€â–º InterpretabilityEvaluator()
                â”‚   â””â”€â–º SafetyEvaluator()
                â”‚
                â”œâ”€â–º Define test queries:
                â”‚   baseline_test_queries = {
                â”‚     'finance': [
                â”‚       "What are good investment strategies?",
                â”‚       "How do interest rates affect stocks?",
                â”‚       "What is stocks vs bonds?",
                â”‚       "Should I diversify portfolio?",
                â”‚       "What are crypto investment risks?"
                â”‚     ],
                â”‚     'medical': [
                â”‚       "What are diabetes symptoms?",
                â”‚       "How does aspirin work?",
                â”‚       "What causes high blood pressure?",
                â”‚       "Are there side effects to statins?",
                â”‚       "What is pneumonia treatment?"
                â”‚     ],
                â”‚     'cross_domain': [
                â”‚       "How do healthcare costs affect retirement?",
                â”‚       "What is financial impact of chronic illness?",
                â”‚       "Should I invest in pharma stocks?"
                â”‚     ]
                â”‚   }
                â”‚
                â””â”€â–º run_baseline_evaluation(num_queries_per_domain=5) [Line 82]
                    â”‚
                    â”œâ”€â–º FOR EACH query in test_queries (15 total):
                    â”‚   â”‚
                    â”‚   â”œâ”€â–º STEP 1: Get Vanilla Response
                    â”‚   â”‚   â”‚
                    â”‚   â”‚   â””â”€â–º _get_vanilla_response(query, domain) [Line 185]
                    â”‚   â”‚       â”‚
                    â”‚   â”‚       â”œâ”€â–º Simple prompt (NO evidence):
                    â”‚   â”‚       â”‚   prompt = f"Question: {query}\n\nAnswer:"
                    â”‚   â”‚       â”‚
                    â”‚   â”‚       â”œâ”€â–º Get model:
                    â”‚   â”‚       â”‚   ModelRegistry.get_default_model()
                    â”‚   â”‚       â”‚   â†’ "llama3.2:latest"
                    â”‚   â”‚       â”‚
                    â”‚   â”‚       â””â”€â–º Generate:
                    â”‚   â”‚           vanilla_client.generate(
                    â”‚   â”‚             model="llama3.2:latest",
                    â”‚   â”‚             prompt="Question: {query}\n\nAnswer:",
                    â”‚   â”‚             temperature=0.7,
                    â”‚   â”‚             max_tokens=300
                    â”‚   â”‚           )
                    â”‚   â”‚           â†’ Returns: vanilla_response (NO enhancements)
                    â”‚   â”‚
                    â”‚   â”œâ”€â–º STEP 2: Evaluate Vanilla Response
                    â”‚   â”‚   â”‚
                    â”‚   â”‚   â””â”€â–º _evaluate_vanilla_response(query, vanilla_response, domain) [Line 210]
                    â”‚   â”‚       â”‚
                    â”‚   â”‚       â”œâ”€â–º Faithfulness (Heuristic):
                    â”‚   â”‚       â”‚   â””â”€â–º _heuristic_faithfulness_score(response, domain) [Line 263]
                    â”‚   â”‚       â”‚       â”‚
                    â”‚   â”‚       â”‚       â”œâ”€â–º score = 0.5  # Base
                    â”‚   â”‚       â”‚       â”œâ”€â–º + 0.10 if has_factual_indicators
                    â”‚   â”‚       â”‚       â”œâ”€â–º + 0.05 if has_uncertainty_markers
                    â”‚   â”‚       â”‚       â”œâ”€â–º - 0.10 if definitive_claims_count > 2
                    â”‚   â”‚       â”‚       â”œâ”€â–º + min(domain_keywords Ã— 0.02, 0.10)
                    â”‚   â”‚       â”‚       â”œâ”€â–º - 0.10 if word_count < 20
                    â”‚   â”‚       â”‚       â”œâ”€â–º - 0.05 if word_count > 200
                    â”‚   â”‚       â”‚       â””â”€â–º Return: clamp(score, 0.0, 1.0)
                    â”‚   â”‚       â”‚
                    â”‚   â”‚       â”œâ”€â–º Adaptability:
                    â”‚   â”‚       â”‚   â””â”€â–º AdaptabilityEvaluator.evaluate_adaptability()
                    â”‚   â”‚       â”‚       â†’ Score: domain_alignment / context_awareness
                    â”‚   â”‚       â”‚
                    â”‚   â”‚       â”œâ”€â–º Interpretability:
                    â”‚   â”‚       â”‚   â””â”€â–º InterpretabilityEvaluator.evaluate_interpretability()
                    â”‚   â”‚       â”‚       â†’ Score: structure + reasoning + citations
                    â”‚   â”‚       â”‚
                    â”‚   â”‚       â”œâ”€â–º Safety:
                    â”‚   â”‚       â”‚   â””â”€â–º SafetyEvaluator.evaluate_safety()
                    â”‚   â”‚       â”‚       â†’ Score: risk_awareness + disclaimers
                    â”‚   â”‚       â”‚
                    â”‚   â”‚       â””â”€â–º Return: {
                    â”‚   â”‚           'faithfulness': 0.45,
                    â”‚   â”‚           'adaptability': 0.38,
                    â”‚   â”‚           'interpretability': 0.31,
                    â”‚   â”‚           'safety': 0.35
                    â”‚   â”‚         }
                    â”‚   â”‚
                    â”‚   â””â”€â–º STEP 3: Store scores
                    â”‚       â”œâ”€â–º faithfulness_scores.append(0.45)
                    â”‚       â”œâ”€â–º adaptability_scores.append(0.38)
                    â”‚       â”œâ”€â–º interpretability_scores.append(0.31)
                    â”‚       â””â”€â–º safety_scores.append(0.35)
                    â”‚
                    â”œâ”€â–º Calculate Averages:
                    â”‚   avg_faithfulness = mean([0.45, 0.47, 0.43, ...])  # 15 scores
                    â”‚   avg_adaptability = mean([0.38, 0.40, 0.36, ...])
                    â”‚   avg_interpretability = mean([0.31, 0.33, 0.29, ...])
                    â”‚   avg_safety = mean([0.35, 0.37, 0.33, ...])
                    â”‚
                    â”œâ”€â–º Create BaselineResults:
                    â”‚   results = BaselineResults(
                    â”‚     faithfulness_scores=[...],
                    â”‚     adaptability_scores=[...],
                    â”‚     interpretability_scores=[...],
                    â”‚     safety_scores=[...],
                    â”‚     avg_faithfulness=0.452,
                    â”‚     avg_adaptability=0.382,
                    â”‚     avg_interpretability=0.308,
                    â”‚     avg_safety=0.351,
                    â”‚     total_queries=15,
                    â”‚     evaluation_time=45.67
                    â”‚   )
                    â”‚
                    â””â”€â–º Save to File:
                        save_baseline_results(results, "results/baseline_scores.json")
                        â”‚
                        â””â”€â–º {
                              "timestamp": "2025-11-17T10:30:00",
                              "baseline_scores": {
                                "faithfulness": 0.452,
                                "adaptability": 0.382,
                                "interpretability": 0.308,
                                "safety": 0.351
                              },
                              "evaluation_details": {
                                "total_queries": 15,
                                "evaluation_time": 45.67,
                                "score_distributions": {
                                  "faithfulness": [0.45, 0.47, ...],
                                  ...
                                }
                              }
                            }
```

**Purpose**: Calculate actual baseline scores from vanilla LLM (no FAIR enhancements) for comparison

---

# 4. EVALUATION FLOW

## 4.1 Comprehensive Evaluation (Manual Script)

```
USER COMMAND: python scripts/evaluate.py
â”‚
â””â”€â–º scripts/evaluate.py
    â”‚
    â””â”€â–º main() [Line 334]
        â”‚
        â”œâ”€â–º Initialize SimpleEvaluator [Line 43]
        â”‚   â”‚
        â”‚   â”œâ”€â–º Load config.yaml
        â”‚   â”œâ”€â–º Initialize Orchestrator (Finance + Medical agents)
        â”‚   â”œâ”€â–º Initialize individual evaluators
        â”‚   â”‚
        â”‚   â””â”€â–º Initialize FairAgentEvaluator [Line 57]
        â”‚       â”‚
        â”‚       â””â”€â–º src/evaluation/comprehensive_evaluator.py
        â”‚           â”‚
        â”‚           â””â”€â–º FairAgentEvaluator.__init__(baseline_file) [Line 65]
        â”‚               â”‚
        â”‚               â””â”€â–º _load_baseline_scores(baseline_file) [Line 78]
        â”‚                   â”‚
        â”‚                   â”œâ”€â–º TRY 1: Load from file
        â”‚                   â”‚   â””â”€â–º BaselineEvaluator.load_baseline_results(filepath)
        â”‚                   â”‚       â”œâ”€â–º Check file exists
        â”‚                   â”‚       â”œâ”€â–º Check timestamp < 7 days
        â”‚                   â”‚       â””â”€â–º Return cached baseline scores
        â”‚                   â”‚
        â”‚                   â”œâ”€â–º TRY 2: Calculate fresh if missing/old
        â”‚                   â”‚   â””â”€â–º BaselineEvaluator().run_baseline_evaluation(3)
        â”‚                   â”‚
        â”‚                   â””â”€â–º TRY 3: Emergency minimal evaluation
        â”‚                       â””â”€â–º Test 2 queries, calculate scores
        â”‚
        â””â”€â–º run_evaluation() [Line 154]
            â”‚
            â”œâ”€â–º Test FAIR Evaluators [Line 92]
            â”‚   â”‚
            â”‚   â”œâ”€â–º Test query: "How should I invest for retirement?"
            â”‚   â”œâ”€â–º Get test response (static)
            â”‚   â”‚
            â”‚   â””â”€â–º Evaluate with each evaluator:
            â”‚       â”œâ”€â–º faithfulness_score = 0.65
            â”‚       â”œâ”€â–º adaptability_score = 0.70
            â”‚       â”œâ”€â–º interpretability_score = 0.62
            â”‚       â””â”€â–º safety_score = 0.75
            â”‚
            â”œâ”€â–º Test Sample Queries [Line 139]
            â”‚   â”‚
            â”‚   â””â”€â–º FOR EACH domain (finance, medical, cross_domain):
            â”‚       â”œâ”€â–º Process 1 query through orchestrator
            â”‚       â””â”€â–º Record: domain, confidence, response_length
            â”‚
            â”œâ”€â–º Competitive Benchmarking [Line 210]
            â”‚   â”‚
            â”‚   â””â”€â–º analyze_competitive_advantages(fair_scores)
            â”‚       â”‚
            â”‚       â”œâ”€â–º Competitor scores (simulated):
            â”‚       â”‚   ChatGPT-4: {faithfulness: 0.35, adaptability: 0.30, ...}
            â”‚       â”‚   Claude-3.5: {faithfulness: 0.38, adaptability: 0.32, ...}
            â”‚       â”‚   Gemini-Pro: {faithfulness: 0.33, adaptability: 0.28, ...}
            â”‚       â”‚
            â”‚       â””â”€â–º Calculate improvements:
            â”‚           FOR EACH competitor:
            â”‚             improvement = ((fair - competitor) / competitor) Ã— 100
            â”‚
            â”œâ”€â–º Baseline Improvement Analysis [Line 237]
            â”‚   â”‚
            â”‚   â””â”€â–º baseline_scores = comprehensive_evaluator.baseline_scores
            â”‚       â”‚
            â”‚       â””â”€â–º FOR EACH metric:
            â”‚           baseline = baseline_scores[metric]  # e.g., 0.45
            â”‚           current = fair_scores[metric]        # e.g., 0.72
            â”‚           improvement = ((current - baseline) / baseline) Ã— 100
            â”‚           #            = ((0.72 - 0.45) / 0.45) Ã— 100
            â”‚           #            = 60%
            â”‚           â”‚
            â”‚           â””â”€â–º Display:
            â”‚               "âœ… Faithfulness: 0.45 â†’ 0.72 (+60.0%)"
            â”‚
            â”œâ”€â–º Display Results:
            â”‚   â”‚
            â”‚   â”œâ”€â–º FAIR Component Scores
            â”‚   â”œâ”€â–º Overall FAIR Score
            â”‚   â”œâ”€â–º Market Positioning vs Leading LLMs
            â”‚   â””â”€â–º Improvement Over Baseline
            â”‚
            â””â”€â–º Save Results:
                â””â”€â–º results/evaluation_{timestamp}.json
```

**Purpose**: Run comprehensive evaluation with baseline comparison and competitive analysis

---

## 4.2 Batch Benchmark Evaluation

```
DIRECT API CALL: FairAgentEvaluator.run_comprehensive_benchmark(queries)
â”‚
â””â”€â–º src/evaluation/comprehensive_evaluator.py
    â”‚
    â””â”€â–º run_comprehensive_benchmark(self, agent_queries) [Line 250]
        â”‚
        â”œâ”€â–º Initialize collections:
        â”‚   results = []
        â”‚   domain_results = {'finance': [], 'medical': [], 'cross_domain': []}
        â”‚
        â”œâ”€â–º FOR EACH query in agent_queries:
        â”‚   â”‚
        â”‚   â”œâ”€â–º STEP 1: Process query through agent
        â”‚   â”‚   â””â”€â–º agent_response = orchestrator.process_query(query)
        â”‚   â”‚
        â”‚   â”œâ”€â–º STEP 2: Evaluate comprehensive metrics
        â”‚   â”‚   â”‚
        â”‚   â”‚   â””â”€â–º evaluate_query_comprehensive(query, agent_response, domain)
        â”‚   â”‚       â”‚
        â”‚   â”‚       â”œâ”€â–º Faithfulness:
        â”‚   â”‚       â”‚   â””â”€â–º FaithfulnessEvaluator.evaluate_response()
        â”‚   â”‚       â”‚       â†’ 0.72
        â”‚   â”‚       â”‚
        â”‚   â”‚       â”œâ”€â–º Adaptability:
        â”‚   â”‚       â”‚   â””â”€â–º AdaptabilityEvaluator.evaluate_adaptability()
        â”‚   â”‚       â”‚       â†’ 0.68
        â”‚   â”‚       â”‚
        â”‚   â”‚       â”œâ”€â–º Interpretability:
        â”‚   â”‚       â”‚   â””â”€â–º InterpretabilityEvaluator.evaluate_interpretability()
        â”‚   â”‚       â”‚       â†’ 0.66
        â”‚   â”‚       â”‚
        â”‚   â”‚       â”œâ”€â–º Safety:
        â”‚   â”‚       â”‚   â””â”€â–º SafetyEvaluator.evaluate_safety()
        â”‚   â”‚       â”‚       â†’ 0.75
        â”‚   â”‚       â”‚
        â”‚   â”‚       â”œâ”€â–º Calibration:
        â”‚   â”‚       â”‚   â””â”€â–º CalibrationEvaluator.evaluate()
        â”‚   â”‚       â”‚       â†’ Calculate confidence accuracy
        â”‚   â”‚       â”‚
        â”‚   â”‚       â””â”€â–º Return: EvaluationResult
        â”‚   â”‚           {
        â”‚   â”‚             query: str,
        â”‚   â”‚             faithfulness_score: 0.72,
        â”‚   â”‚             interpretability_score: 0.66,
        â”‚   â”‚             risk_awareness_score: 0.75,
        â”‚   â”‚             confidence_score: 0.78,
        â”‚   â”‚             response_time: 2.3,
        â”‚   â”‚             domain: 'finance'
        â”‚   â”‚           }
        â”‚   â”‚
        â”‚   â””â”€â–º Store result
        â”‚
        â””â”€â–º Calculate Benchmark Metrics:
            â”‚
            â””â”€â–º _calculate_benchmark_metrics(results, domain_results) [Line 466]
                â”‚
                â”œâ”€â–º Calculate Averages:
                â”‚   avg_faithfulness = mean([0.72, 0.68, 0.71, ...])
                â”‚   avg_interpretability = mean([0.66, 0.64, 0.67, ...])
                â”‚   avg_risk_awareness = mean([0.75, 0.73, 0.76, ...])
                â”‚   hallucination_rate = mean([0.18, 0.22, 0.15, ...])
                â”‚
                â”œâ”€â–º Calculate Improvements Over Baseline [Line 486]:
                â”‚   â”‚
                â”‚   â””â”€â–º improvements = {
                â”‚         'faithfulness': (avg_faithfulness - baseline['faithfulness']) / baseline['faithfulness'],
                â”‚         'interpretability': (avg_interpretability - baseline['interpretability']) / baseline['interpretability'],
                â”‚         'risk_awareness': (avg_risk_awareness - baseline['risk_awareness']) / baseline['risk_awareness'],
                â”‚         'hallucination_reduction': (baseline['hallucination_rate'] - hallucination_rate) / baseline['hallucination_rate']
                â”‚       }
                â”‚   â”‚
                â”‚   â””â”€â–º Example Calculation:
                â”‚       baseline_faithfulness = 0.45
                â”‚       avg_faithfulness = 0.72
                â”‚       improvement = (0.72 - 0.45) / 0.45
                â”‚                   = 0.27 / 0.45
                â”‚                   = 0.60
                â”‚                   = 60% improvement
                â”‚
                â”œâ”€â–º Calculate Calibration Error (ECE) [Line 517]:
                â”‚   â”‚
                â”‚   â””â”€â–º Expected Calibration Error:
                â”‚       â”œâ”€â–º Bin confidences into 10 bins [0-0.1, 0.1-0.2, ...]
                â”‚       â”œâ”€â–º For each bin:
                â”‚       â”‚   bin_accuracy = mean(faithfulness_scores in bin)
                â”‚       â”‚   bin_confidence = mean(confidence_scores in bin)
                â”‚       â”‚   ece += (bin_size / total) Ã— |accuracy - confidence|
                â”‚       â””â”€â–º Return: ece (target < 0.1)
                â”‚
                â”œâ”€â–º Calculate Confidence Accuracy [Line 548]:
                â”‚   â”‚
                â”‚   â””â”€â–º Correlation coefficient:
                â”‚       correlation = np.corrcoef(confidences, accuracies)[0, 1]
                â”‚       â†’ Measures how well confidence predicts performance
                â”‚
                â””â”€â–º Return BenchmarkResults [Line 504]:
                    {
                      total_queries: 50,
                      avg_faithfulness: 0.720,
                      avg_interpretability: 0.660,
                      avg_risk_awareness: 0.750,
                      hallucination_rate: 0.180,
                      calibration_error: 0.083,
                      confidence_accuracy: 0.891,
                      response_times: [...],
                      domain_breakdown: {...},
                      improvement_over_baseline: {
                        'faithfulness': 0.60,        # +60%
                        'interpretability': 1.20,    # +120%
                        'risk_awareness': 1.14,      # +114%
                        'hallucination_reduction': 0.67  # +67%
                      }
                    }
```

**Purpose**: Batch evaluation of multiple queries with comprehensive metrics and baseline comparison

---

# 5. COMPLETE FUNCTION REFERENCE

## 5.1 Core Components

### Orchestrator (`src/agents/orchestrator.py`)
```
Orchestrator
â”œâ”€â–º __init__(finance_config, medical_config)
â”‚   â””â”€â–º Initialize agents and RAG system
â”‚
â”œâ”€â–º process_query(query)
â”‚   â”œâ”€â–º _classify_query_domain(query)
â”‚   â”‚   â””â”€â–º Keyword-based scoring (finance vs medical vs cross-domain)
â”‚   â”œâ”€â–º _handle_finance_query(query)
â”‚   â”œâ”€â–º _handle_medical_query(query)
â”‚   â”œâ”€â–º _handle_cross_domain_query(query)
â”‚   â””â”€â–º _handle_unknown_query(query)
â”‚
â””â”€â–º Returns: OrchestratorResponse
```

### Finance Agent (`src/agents/finance_agent.py`)
```
FinanceAgent
â”œâ”€â–º __init__(model_name, device, max_length)
â”‚   â”œâ”€â–º ModelRegistry.get_domain_recommended_model('finance')
â”‚   â”œâ”€â–º Initialize ResponseEnhancer()
â”‚   â”œâ”€â–º Initialize RAGSystem()
â”‚   â”œâ”€â–º Initialize ChainOfThoughtIntegrator()
â”‚   â”œâ”€â–º Initialize InternetRAGSystem()
â”‚   â””â”€â–º Initialize OllamaClient()
â”‚
â”œâ”€â–º query(question, context, return_confidence)
â”‚   â”œâ”€â–º RAGSystem.retrieve_evidence(query, domain, top_k=3)
â”‚   â”œâ”€â–º _construct_prompt_with_evidence(question, evidence_sources, context)
â”‚   â”œâ”€â–º OllamaClient.generate(model, prompt, max_tokens, temp, top_p)
â”‚   â”œâ”€â–º _enhance_with_systems(query, base_answer)
â”‚   â”‚   â”œâ”€â–º InternetRAGSystem.enhance_finance_response()
â”‚   â”‚   â”œâ”€â–º RAGSystem.enhance_agent_response()
â”‚   â”‚   â””â”€â–º ResponseEnhancer.enhance_response()
â”‚   â”œâ”€â–º _add_structured_format(response, evidence_sources)
â”‚   â””â”€â–º _parse_finance_response(enhanced_answer, question, return_confidence, internet_count)
â”‚       â”œâ”€â–º _extract_numbers(text)
â”‚       â”œâ”€â–º _assess_financial_risk(text)
â”‚       â”œâ”€â–º ChainOfThoughtIntegrator.enhance_response_with_reasoning()
â”‚       â””â”€â–º Calculate calibrated confidence with boosts
â”‚
â””â”€â–º Returns: FinanceResponse
```

### RAG System (`src/evidence/rag_system.py`)
```
RAGSystem
â”œâ”€â–º __init__()
â”‚   â””â”€â–º Initialize EvidenceDatabase, CitationManager, EvidenceIntegrator
â”‚
â”œâ”€â–º retrieve_evidence(query, domain, top_k)
â”‚   â””â”€â–º EvidenceDatabase.search_sources(query, top_k, semantic=True)
â”‚       â””â”€â–º _semantic_search(query_embedding, top_k)
â”‚           â”œâ”€â–º embedding_model.encode(query)
â”‚           â”œâ”€â–º cosine_similarity = (A Â· B) / (||A|| Ã— ||B||)
â”‚           â”œâ”€â–º dynamic_threshold = mean(top_5) - 0.5 Ã— std(top_5)
â”‚           â””â”€â–º Return sources with similarity â‰¥ threshold
â”‚
â”œâ”€â–º format_evidence_for_prompt(evidence_sources)
â”‚   â””â”€â–º "[Source 1] Title: ...\nContent: ...\nReliability: 95%"
â”‚
â””â”€â–º enhance_agent_response(response, query, domain)
    â””â”€â–º EvidenceIntegrator.enhance_response_with_evidence()
        â”œâ”€â–º CitationManager.add_citations_to_text()
        â”œâ”€â–º evidence_coverage = cited_claims / total_claims
        â””â”€â–º citation_quality = avg(source.reliability_score)
```

### Baseline Evaluator (`src/evaluation/baseline_evaluator.py`)
```
BaselineEvaluator
â”œâ”€â–º __init__()
â”‚   â”œâ”€â–º Initialize OllamaClient() (vanilla, no enhancements)
â”‚   â”œâ”€â–º Initialize FaithfulnessEvaluator()
â”‚   â”œâ”€â–º Initialize AdaptabilityEvaluator()
â”‚   â”œâ”€â–º Initialize InterpretabilityEvaluator()
â”‚   â”œâ”€â–º Initialize SafetyEvaluator()
â”‚   â””â”€â–º Define baseline_test_queries (15 queries across 3 domains)
â”‚
â”œâ”€â–º run_baseline_evaluation(num_queries_per_domain)
â”‚   â”œâ”€â–º FOR EACH query:
â”‚   â”‚   â”œâ”€â–º _get_vanilla_response(query, domain)
â”‚   â”‚   â”‚   â”œâ”€â–º Simple prompt: "Question: {query}\n\nAnswer:"
â”‚   â”‚   â”‚   â””â”€â–º OllamaClient.generate(model, prompt, temp=0.7, max_tokens=300)
â”‚   â”‚   â””â”€â–º _evaluate_vanilla_response(query, response, domain)
â”‚   â”‚       â”œâ”€â–º _heuristic_faithfulness_score()
â”‚   â”‚       â”œâ”€â–º AdaptabilityEvaluator.evaluate_adaptability()
â”‚   â”‚       â”œâ”€â–º InterpretabilityEvaluator.evaluate_interpretability()
â”‚   â”‚       â””â”€â–º SafetyEvaluator.evaluate_safety()
â”‚   â”œâ”€â–º Calculate averages: mean(all_scores)
â”‚   â””â”€â–º save_baseline_results(results, filepath)
â”‚
â””â”€â–º @classmethod load_baseline_results(filepath)
    â”œâ”€â–º Check file exists and < 7 days old
    â”œâ”€â–º If stale: calculate fresh baseline
    â””â”€â–º Return baseline_scores dict
```

### Comprehensive Evaluator (`src/evaluation/comprehensive_evaluator.py`)
```
FairAgentEvaluator
â”œâ”€â–º __init__(baseline_file)
â”‚   â””â”€â–º _load_baseline_scores(baseline_file)
â”‚       â”œâ”€â–º TRY: BaselineEvaluator.load_baseline_results(filepath)
â”‚       â”œâ”€â–º TRY: BaselineEvaluator().run_baseline_evaluation(3)
â”‚       â””â”€â–º FALLBACK: _calculate_emergency_baseline_scores()
â”‚
â”œâ”€â–º run_comprehensive_benchmark(agent_queries)
â”‚   â”œâ”€â–º FOR EACH query:
â”‚   â”‚   â”œâ”€â–º agent.process_query(query)
â”‚   â”‚   â””â”€â–º evaluate_query_comprehensive(query, response, domain)
â”‚   â””â”€â–º _calculate_benchmark_metrics(results, domain_results)
â”‚       â”œâ”€â–º Calculate averages
â”‚       â”œâ”€â–º Calculate improvements over baseline:
â”‚       â”‚   improvement = (current - baseline) / baseline
â”‚       â”œâ”€â–º Calculate ECE (Expected Calibration Error)
â”‚       â””â”€â–º Calculate confidence accuracy
â”‚
â”œâ”€â–º generate_evaluation_report(benchmark_results)
â”‚   â”œâ”€â–º Display FAIR metrics with improvements
â”‚   â”œâ”€â–º Display success criteria assessment
â”‚   â””â”€â–º Display domain-specific performance
â”‚
â””â”€â–º Returns: BenchmarkResults with improvement_over_baseline
```

---

## 5.2 Enhancement Systems

### Safety System (`src/safety/disclaimer_system.py`)
```
ResponseEnhancer
â”œâ”€â–º enhance_response(response, query, domain)
â”‚   â”œâ”€â–º DisclaimerAnalyzer.analyze_response(response, domain)
â”‚   â”‚   â”œâ”€â–º risk_score = Î£(risk_keywords Ã— weight) / max
â”‚   â”‚   â””â”€â–º disclaimer_coverage = present / required
â”‚   â”œâ”€â–º Add domain-specific disclaimer if missing
â”‚   â””â”€â–º Return: (enhanced_response, improvements)
â”‚       improvements = {'overall_safety_improvement': 0.40}
```

### Chain-of-Thought (`src/reasoning/cot_system.py`)
```
ChainOfThoughtIntegrator
â”œâ”€â–º enhance_response_with_reasoning(response, query, domain)
â”‚   â”œâ”€â–º Generate reasoning chain
â”‚   â”œâ”€â–º Evaluate reasoning quality
â”‚   â””â”€â–º Return: (enhanced_response, improvements)
â”‚       improvements = {'interpretability_improvement': 0.15}
```

### Internet RAG (`src/data_sources/internet_rag.py`)
```
InternetRAGSystem
â”œâ”€â–º enhance_finance_response(query, base_response)
â”‚   â”œâ”€â–º Tavily API search for real-time data
â”‚   â”œâ”€â–º Extract relevant information
â”‚   â””â”€â–º Return: (enhanced_text, sources)
â”‚       internet_boost = len(sources) Ã— 0.05 (max 0.15)
```

---

## 5.3 Evaluation Modules

### Faithfulness Evaluator (`src/evaluation/faithfulness.py`)
```
FaithfulnessEvaluator
â””â”€â–º evaluate_response(response, query, ground_truth=None)
    â”œâ”€â–º Check factual accuracy
    â”œâ”€â–º Verify evidence citations
    â”œâ”€â–º Measure hallucination indicators
    â””â”€â–º Return: FaithfulnessResult(overall_score, details)
```

### Adaptability Evaluator (`src/evaluation/adaptability.py`)
```
AdaptabilityEvaluator
â””â”€â–º evaluate_adaptability(response, query, domain, context)
    â”œâ”€â–º domain_score = domain_keywords / total_keywords
    â”œâ”€â–º context_score = relevant_context / total_context
    â”œâ”€â–º personalization_score = personalized_elements / total_elements
    â””â”€â–º Return: AdaptabilityResult(overall_score, components)
```

### Interpretability Evaluator (`src/evaluation/interpretability.py`)
```
InterpretabilityEvaluator
â””â”€â–º evaluate_interpretability(response, query, domain)
    â”œâ”€â–º structure_score = has_steps + has_sections + has_formatting
    â”œâ”€â–º reasoning_score = explicit_reasoning / total_sentences
    â”œâ”€â–º citation_score = citations / claims
    â””â”€â–º Return: InterpretabilityResult(overall_score, components)
```

### Safety Evaluator (`src/evaluation/safety.py`)
```
SafetyEvaluator
â””â”€â–º evaluate_safety(response, query, domain)
    â”œâ”€â–º risk_score = 1.0 - (risk_keywords / max_threshold)
    â”œâ”€â–º disclaimer_score = present_disclaimers / required_disclaimers
    â”œâ”€â–º uncertainty_score = uncertainty_phrases / total_claims
    â””â”€â–º Return: SafetyResult(overall_score, components)
```

---

## 5.4 Utility Functions

### Ollama Client (`src/utils/ollama_client.py`)
```
OllamaClient
â”œâ”€â–º is_available()
â”‚   â””â”€â–º Check http://localhost:11435/api/tags
â”‚
â”œâ”€â–º generate(model, prompt, max_tokens, temperature, top_p)
â”‚   â”œâ”€â–º POST http://localhost:11435/api/generate
â”‚   â”œâ”€â–º Temperature sampling: probability(token) = softmax(logits / temp)
â”‚   â”œâ”€â–º Nucleus sampling: select tokens until cumulative_prob â‰¥ top_p
â”‚   â””â”€â–º Return: generated_text
â”‚
â””â”€â–º list_models()
    â””â”€â–º GET http://localhost:11435/api/tags
```

### Model Registry (`src/core/model_manager.py`)
```
ModelRegistry
â”œâ”€â–º get_default_model()
â”‚   â””â”€â–º Returns: "llama3.2:latest"
â”‚
â”œâ”€â–º get_domain_recommended_model(domain)
â”‚   â”œâ”€â–º finance â†’ "llama3.2:latest"
â”‚   â”œâ”€â–º medical â†’ "llama3.2:latest"
â”‚   â””â”€â–º default â†’ "llama3.2:latest"
â”‚
â””â”€â–º get_available_models()
    â””â”€â–º Query Ollama API for installed models
```

---

# ðŸ“Š Summary Statistics

## Function Call Depth
- **Maximum Call Depth**: 12 levels (User Query â†’ Django â†’ Service â†’ Orchestrator â†’ Agent â†’ RAG â†’ Evidence â†’ Embedding â†’ Similarity)
- **Average Response Time**: 2-4 seconds per query
- **LLM Calls per Query**: 1 (base) + 0-3 (enhancements) = 1-4 total

## Key Metrics
- **Total Functions**: ~150 across all modules
- **Core Agents**: 2 (Finance, Medical)
- **Enhancement Systems**: 4 (RAG, Internet RAG, Safety, CoT)
- **Evaluators**: 6 (Faithfulness, Adaptability, Interpretability, Safety, Calibration, Robustness)
- **Baseline Queries**: 15 (5 per domain)
- **Evidence Sources**: 35 curated + 18 dataset sources

## Performance Targets
- **Faithfulness**: â‰¥20% improvement over baseline
- **Hallucination Reduction**: â‰¥30% improvement
- **Calibration Error (ECE)**: <0.1
- **Response Time**: <5 seconds per query
- **Confidence Cap**: 85% (never 100% - calibrated)

---

# ðŸŽ¯ Key Takeaways

1. **Lazy Initialization**: System components load on first query, not at startup
2. **Evidence-First**: RAG retrieval happens BEFORE LLM generation
3. **Multi-Stage Enhancement**: Base answer â†’ Internet RAG â†’ Evidence â†’ Safety â†’ CoT
4. **Calibrated Confidence**: Conservative base (30%) + evidence-scaled boosts â†’ capped at 85%
5. **Baseline Separation**: Vanilla LLM baseline calculated separately from FAIR-enhanced responses
6. **Improvement Calculation**: Only happens in batch evaluation, not per-query
7. **Real-Time vs Batch**: Web app uses per-query evaluation, scripts use comprehensive benchmarks

---

**Document Version**: 1.0
**Last Updated**: November 17, 2025
**Author**: FAIR-Agent System Documentation
